{"cells":[{"cell_type":"code","source":["%sql\nselect percentile(cnt,0.25) as cnt_25,\n       percentile(cnt,0.5) as cnt_50,\n       percentile(cnt,0.75) as cnt_75,\n       percentile(cnt,0.9) as cnt_90,\n       percentile(cnt,0.95) as cnt_95,\n       percentile(cnt,0.99) as cnt_99,\n       percentile(cnt,0.995) as cnt_995,\n       percentile(cnt,0.999) as cnt_999\nfrom (\n  select uid, count(*) as cnt\n  from play_sid_sl\n  group by uid\n  )\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["robots=sqlContext.sql(\"\"\"\nselect uid\nfrom play_sid_sl\ngroup by uid\nhaving count(*)>2489\n\"\"\")\nrobots.write.saveAsTable(\"robots\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["play_filtered=sqlContext.sql(\"\"\"\nselect *\nfrom play_sid_sl\nwhere uid not in (\n select uid\n from robots\n )\n\"\"\")\nplay_filtered.write.saveAsTable(\"play_filtered\")\nsqlContext.cacheTable(\"play_filtered\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["down_filtered=sqlContext.sql(\"\"\"\nselect *\nfrom down_sid\nwhere uid not in (\n select uid\n from robots\n )\n\"\"\")\ndown_filtered.write.saveAsTable(\"down_filtered\")\n#sqlContext.cacheTable(\"down_filtered\")"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%sql\nselect count(*) as num_rows, count(distinct uid) as num_users, count(distinct sid) as num_songs\nfrom play_filtered"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%sql\nselect count(*) as num_rows, count(distinct uid) as num_users, count(distinct sid) as num_songs\nfrom down_filtered"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%sql\nselect weekofyear(date),min(date), max(date), count(*)\nfrom play_filtered\ngroup by weekofyear(date)\norder by weekofyear(date)\n--week 13-18 are whole weeks"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%sql\nselect percentile(play_time/song_length*1.0,0.25) as times_25,\n       percentile(play_time/song_length*1.0,0.5) as times_50,\n       percentile(play_time/song_length*1.0,0.75) as times_75,\n       percentile(play_time/song_length*1.0,0.9) as times_90,\n       percentile(play_time/song_length*1.0,0.95) as times_95\nfrom play_filtered\nwhere date<\"2017-05-08\" and date>\"2017-03-28\""],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%sql --cumulative play times\nselect percentile(cnt,0.25) as cnt_25, percentile(cnt,0.5) as cnt_50, percentile(cnt,0.75) as cnt_75, percentile(cnt,0.90) as cnt_90, percentile(cnt,0.925) as cnt_93, percentile(cnt,0.95) as cnt_95, percentile(cnt,0.975) as cnt_975, percentile(cnt,0.99) as cnt_99, percentile(cnt,0.999) as cnt_999\nfrom (\n  select uid, sid, sum(case when play_time/song_length*1.0>=0.8 then 1 else 0 end) as cnt\n  from play_filtered\n  group by uid, sid\n  )"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%sql\nselect score,count(*) \nfrom (\nselect uid, sid, \n       case when sum(case when play_time/song_length*1.0>=0.8 then 1 else 0 end)<2 then 0\n            when sum(case when play_time/song_length*1.0>=0.8 then 1 else 0 end)<4 then 1\n            when sum(case when play_time/song_length*1.0>=0.8 then 1 else 0 end)<7 then 2\n            else 3 end as score\nfrom play_filtered\ngroup by uid, sid\n  )\ngroup by score\norder by score"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["from pyspark.ml.evaluation import Evaluator\n\nclass eprEvaluator(Evaluator):    \n  def _evaluate(self, predictions):\n    predictions.createOrReplaceTempView(\"predictions\")\n    result = spark.sql(\"\"\"\n      select sum(prank*rating)/sum(rating) as expected_prank\n      from (\n         select uid, sid, rating, \n               (rank() over (partition by uid order by prediction desc)-1)*1.0/(count(sid) over (partition by uid)-1) as prank\n         from predictions\n          )\n    \"\"\").collect()[0][0]    \n    return float(result)\n  "],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["def pop_fit(ratings_mat):\n    ratings_mat.createOrReplaceTempView(\"ratings_mat\")\n    model = spark.sql(\"\"\"\n      select sid, count(*) as prediction\n      from ratings_mat\n      group by sid\n    \"\"\")   \n    return model\n      \ndef pop_transform(model, test):\n    model.createOrReplaceTempView(\"model\")\n    test.createOrReplaceTempView(\"test\")\n    predictions = spark.sql(\"\"\"\n      select t.*, m.prediction\n      from test t left join\n           model m\n           on t.sid=m.sid\n    \"\"\")   \n    return predictions"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["def bestepr(test):\n    test.createOrReplaceTempView(\"test\")\n    result = spark.sql(\"\"\"\n      select sum(prank*rating)/sum(rating) as expected_prank\n      from (\n         select uid, sid, rating, \n               (rank() over (partition by uid order by rating desc)-1)*1.0/(count(sid) over (partition by uid)-1) as prank\n         from test\n          )\n    \"\"\").collect()[0][0]    \n    return float(result)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["from pyspark.ml.recommendation import ALS\nfrom pyspark.ml.evaluation import RegressionEvaluator"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Build the recommendation model on week13-17 data\nrating_im_train=spark.sql(\"\"\"\nselect p.uid, p.sid, (case when (d.uid is not null and d.sid is not null) then cnt+1 else cnt end) as rating\nfrom (\n  select uid, sid, sum(case when play_time/song_length*1.0>=0.8 then 1 else 0 end) as cnt\n  from play_filtered\n  where weekofyear(date)<18 and weekofyear(date)>12\n  group by uid, sid\n      )p left join\n      (\n      select *\n      from down_filtered\n      where weekofyear(date)<18 and weekofyear(date)>12\n      )d\n      on p.uid=d.uid and p.sid=d.sid\n\"\"\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Evaluate the model by computing the expected percentile ranking on week18 data\nrating_im_test=spark.sql(\"\"\"\nselect p.uid, p.sid, (case when (d.uid is not null and d.sid is not null) then cnt+1 else cnt end) as rating\nfrom (\n  select uid, sid, sum(case when play_time/song_length*1.0>=0.8 then 1 else 0 end) as cnt\n  from play_filtered\n  where weekofyear(date)=18\n  group by uid, sid\n      )p left join\n      (\n      select *\n      from down_filtered\n      where weekofyear(date)=18\n      )d\n      on p.uid=d.uid and p.sid=d.sid\n\"\"\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["model_pop=pop_fit(rating_im_train)\npredictions_pop=pop_transform(model_pop, rating_im_test)\nepr_evaluator = eprEvaluator()\nepr_pop = epr_evaluator.evaluate(predictions_pop)\nprint(\"Expected percentile ranking for popularity recommendation = \" + str(epr_pop))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["best_epr = bestepr(rating_im_test)\nprint(\"The best percentile ranking possible =\" + str(best_epr))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["als_im= ALS(alpha=30, maxIter=5, rank=50, regParam=0.1, userCol=\"uid\", itemCol=\"sid\", ratingCol=\"rating\", coldStartStrategy=\"drop\",                         implicitPrefs=True, nonnegative=False)\n\nmodel_im = als_im.fit(rating_im_train)\n\npredictions_im = model_im.transform(rating_im_test)\n\nepr_evaluator = eprEvaluator()\nepr_im = epr_evaluator.evaluate(predictions_im)\nprint(\"Expected percentile ranking for implicit rating = \" + str(epr_im))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["als_im= ALS(alpha=200, maxIter=7, rank=50, regParam=0.08, userCol=\"uid\", itemCol=\"sid\", ratingCol=\"rating\", coldStartStrategy=\"drop\",                         implicitPrefs=True, nonnegative=False)\n\nmodel_im = als_im.fit(rating_im_train)\n\npredictions_im = model_im.transform(rating_im_test)\n\nepr_evaluator = eprEvaluator()\nepr_im = epr_evaluator.evaluate(predictions_im)\nprint(\"Expected percentile ranking for implicit rating = \" + str(epr_im))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# Generate top 10 movie recommendations for each user\nuserRecs_im = model_im.recommendForAllUsers(10)\n# Generate top 10 user recommendations for each song\nsongRecs_im = model_im.recommendForAllItems(10)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["rating_im=spark.sql(\"\"\"\n   select p.uid, p.sid, (case when (d.uid is not null and d.sid is not null) then cnt+1 else cnt end) as rating\n   from (\n         select uid, sid, sum(case when play_time/song_length*1.0>=0.8 then 1 else 0 end) as cnt\n         from play_filtered\n         group by uid, sid\n         )p left join \n         down_filtered d\n         on p.uid=d.uid and p.sid=d.sid\n      \"\"\")\n(rating_im_tr, rating_im_te) = rating_im.randomSplit([0.8, 0.2],seed=50)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["model_pop=pop_fit(rating_im_tr)\npredictions_pop=pop_transform(model_pop, rating_im_te)\nepr_evaluator = eprEvaluator()\nepr_pop = epr_evaluator.evaluate(predictions_pop)\nprint(\"Expected percentile ranking for popularity recommendation = \" + str(epr_pop))\n\nbest_epr = bestepr(rating_im_te)\nprint(\"The best percentile ranking possible =\" + str(best_epr))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["als_im= ALS(alpha=30, maxIter=5, rank=50, regParam=0.1, userCol=\"uid\", itemCol=\"sid\", ratingCol=\"rating\", coldStartStrategy=\"drop\",                         implicitPrefs=True, nonnegative=False)\n\nmodel_im_r = als_im.fit(rating_im_tr)\n\npredictions_im_r = model_im.transform(rating_im_te)\nepr_evaluator = eprEvaluator()\nepr_im_r = epr_evaluator.evaluate(predictions_im_r)\nprint(\"Expected percentile ranking for implicit rating - random split = \" + str(epr_im_r))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["als_im= ALS(alpha=100, maxIter=7, rank=50, regParam=0.1, userCol=\"uid\", itemCol=\"sid\", ratingCol=\"rating\", coldStartStrategy=\"drop\",                         implicitPrefs=True, nonnegative=False)\n\nmodel_im_r = als_im.fit(rating_im_tr)\n\npredictions_im_r = model_im.transform(rating_im_te)\nepr_evaluator = eprEvaluator()\nepr_im_r = epr_evaluator.evaluate(predictions_im_r)\nprint(\"Expected percentile ranking for implicit rating - random split = \" + str(epr_im_r))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["rating_ex=spark.sql(\"\"\"\n  select p.uid, p.sid, least(case when (d.uid is not null and d.sid is not null) then score+1 else score end, 3) as rating\n  from (\n    select uid, sid, \n           case when sum(case when play_time/song_length*1.0>=0.8 then 1 else 0 end)<2 then 0\n                when sum(case when play_time/song_length*1.0>=0.8 then 1 else 0 end)<4 then 1\n                when sum(case when play_time/song_length*1.0>=0.8 then 1 else 0 end)<7 then 2\n                else 3 end as score\n    from play_filtered\n    group by uid, sid\n        )p left join \n        down_filtered d\n        on p.uid=d.uid and p.sid=d.sid \n        \"\"\")\n\n# Randomly split the dataset to train:test as 0.8:0.2; random seed=20\n(rating_ex_train, rating_ex_test) = rating_ex.randomSplit([0.8, 0.2],seed=20)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["model_pop=pop_fit(rating_ex_train)\npredictions_pop=pop_transform(model_pop, rating_ex_test)\nepr_evaluator = eprEvaluator()\nepr_pop = epr_evaluator.evaluate(predictions_pop)\nprint(\"Expected percentile ranking for popularity recommendation = \" + str(epr_pop))\n\nbest_epr = bestepr(rating_ex_test)\nprint(\"The best percentile ranking possible =\" + str(best_epr))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["als_ex = ALS(rank=50, maxIter=7, regParam=0.06, userCol=\"uid\", itemCol=\"sid\", ratingCol=\"rating\", coldStartStrategy=\"drop\",                                    implicitPrefs=False, nonnegative=False)\nmodel_ex = als_ex.fit(rating_ex_train)\n\npredictions_ex = model_ex.transform(rating_ex_test)\n\nrmse_evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\nrmse_ex = rmse_evaluator.evaluate(predictions_ex)\nepr_evaluator = eprEvaluator()\nepr_ex = epr_evaluator.evaluate(predictions_ex)\n\nprint(\"RMSE for explicit rating = \" + str(rmse_ex))\nprint (\"Expected percentile ranking for explicit rating = \" + str(epr_ex))"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["als_ex = ALS(rank=50, maxIter=7, regParam=0.1, userCol=\"uid\", itemCol=\"sid\", ratingCol=\"rating\", coldStartStrategy=\"drop\",                                    implicitPrefs=False, nonnegative=False)\nmodel = als_ex.fit(rating_ex_train)\n\npredictions = model.transform(rating_ex_test)\n\nrmse_evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\nrmse = rmse_evaluator.evaluate(predictions)\nepr_evaluator = eprEvaluator()\nepr = epr_evaluator.evaluate(predictions)\n\nprint(\"RMSE for explicit rating = \" + str(rmse))\nprint (\"Expected percentile ranking = \" + str(epr))"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["#Grid search with TestValidationSplit\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n\nals_ex= ALS(userCol=\"uid\", itemCol=\"sid\", ratingCol=\"rating\", implicitPrefs=False, coldStartStrategy=\"drop\", maxIter=1, rank=5)\ngrid=ParamGridBuilder().addGrid(als_ex.regParam, [0.03,0.06]).build()\n#.addGrid(als_ex.regParam, [0.03,0.06,0.09])\nrmse_evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n                                predictionCol=\"prediction\")\n\ntvs = TrainValidationSplit(estimator=als_ex, estimatorParamMaps=grid, evaluator=rmse_evaluator,trainRatio=0.8,seed=100)\ntvsModel = tvs.fit(rating_ex_train)\npredictions = tvsModel.transform(rating_ex_test)\n\nbest_model= tvsModel.bestModel\nprint (\"The rank for best model is \" + str(best_model.rank))\nprint (\"The Max number of iteration for best model is \" + str(best_model._java_obj.parent().getMaxIter()))"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# Generate top 10 movie recommendations for each user\nuserRecs_ex = model_ex.recommendForAllUsers(10)\n# Generate top 10 user recommendations for each song\nsongRecs_ex = model_ex.recommendForAllItems(10)"],"metadata":{},"outputs":[],"execution_count":31}],"metadata":{"name":"MusicBox-Part II","notebookId":1737319951745432},"nbformat":4,"nbformat_minor":0}
